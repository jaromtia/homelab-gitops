# Homelab Infrastructure - Docker Compose Configuration
# This file orchestrates the complete homelab infrastructure with Cloudflare tunnel access,
# proper networking, service dependencies, and health checks

services:
  # =============================================================================
  # SECURE EXTERNAL ACCESS - LOCALLY MANAGED CLOUDFLARE TUNNEL
  # =============================================================================
  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: cloudflared
    restart: unless-stopped
    
    # Network configuration - connected to frontend network for service access
    networks:
      frontend:
        ipv4_address: 172.20.0.10
    
    # Volume configuration for tunnel credentials and configuration
    volumes:
      - ./config/cloudflared:/etc/cloudflared:ro
      - cloudflared_logs:/var/log
      - /etc/localtime:/etc/localtime:ro
    
    # Enhanced command with configuration validation and metrics
    command: >
      tunnel
      --config /etc/cloudflared/config.yml
      --metrics 0.0.0.0:8080
      --loglevel ${TUNNEL_LOGLEVEL:-info}
      --logfile /var/log/cloudflared.log
      --grace-period ${TUNNEL_GRACE_PERIOD:-30s}
      --retries ${TUNNEL_RETRIES:-10}
      run
    
    # Comprehensive health checks for tunnel connectivity
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/metrics"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s
    
    # Enhanced resource limits and restart policies for reliability
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
        reservations:
          memory: 64M
          cpus: '0.1'
      restart_policy:
        condition: on-failure
        delay: 15s
        max_attempts: 5
        window: 300s
    
    # Environment variables for tunnel configuration
    environment:
      - TUNNEL_METRICS_ADDRESS=${TUNNEL_METRICS_ADDRESS:-0.0.0.0:8080}
      - TUNNEL_LOGLEVEL=${TUNNEL_LOGLEVEL:-info}
      - TUNNEL_LOGFILE=${TUNNEL_LOGFILE:-/var/log/cloudflared.log}
      - TUNNEL_GRACE_PERIOD=${TUNNEL_GRACE_PERIOD:-30s}
      - TUNNEL_RETRIES=${TUNNEL_RETRIES:-10}
      - CLOUDFLARE_TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
      - CLOUDFLARE_TUNNEL_ID=${CLOUDFLARE_TUNNEL_ID}
      - CLOUDFLARE_ACCOUNT_TAG=${CLOUDFLARE_ACCOUNT_TAG}
    
    # Expose metrics port for monitoring integration
    ports:
      - "8080:8080"  # Metrics endpoint for Prometheus scraping
    
    # Service dependencies - ensure core services are healthy before tunnel starts
    depends_on:
      homer:
        condition: service_healthy
      grafana:
        condition: service_healthy
      prometheus:
        condition: service_healthy
      portainer:
        condition: service_healthy
    
    # Labels for service discovery and monitoring
    labels:
      - "homelab.service.type=tunnel"
      - "homelab.service.critical=true"
      - "homelab.monitoring.scrape=true"
      - "homelab.monitoring.port=8080"
      - "homelab.monitoring.path=/metrics"
      - "homelab.network.frontend=true"

  # =============================================================================
  # MONITORING STACK
  # =============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    
    networks:
      - frontend
      - monitoring
      - backend
    
    ports:
      - "9090:9090"
    
    volumes:
      - ./config/prometheus:/etc/prometheus:ro
      - prometheus_data:/prometheus
      - /var/run/docker.sock:/var/run/docker.sock:ro
    
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION:-15d}'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 15s
        max_attempts: 3
        window: 180s

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    
    networks:
      - frontend
      - monitoring
    
    ports:
      - "3000:3000"
    
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
      - GF_SERVER_ROOT_URL=https://grafana.${DOMAIN}
    
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
    
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 15s
        max_attempts: 3
        window: 180s

  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: unless-stopped
    
    networks:
      - monitoring
    
    ports:
      - "3100:3100"
    
    volumes:
      - ./config/loki:/etc/loki:ro
      - loki_data:/loki
    
    command: -config.file=/etc/loki/loki.yml
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 180s

  promtail:
    image: grafana/promtail:latest
    container_name: promtail
    restart: unless-stopped
    
    networks:
      - monitoring
    
    volumes:
      - ./config/promtail:/etc/promtail:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    
    command: -config.file=/etc/promtail/promtail-minimal.yml
    
    depends_on:
      loki:
        condition: service_healthy
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9080/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 180s

  # Node Exporter for system metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    
    networks:
      - monitoring
    
    ports:
      - "9100:9100"
    
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($|/)'
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9100/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 180s

  # cAdvisor for container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    restart: unless-stopped
    
    networks:
      - monitoring
    
    ports:
      - "8081:8080"
    
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    
    devices:
      - /dev/kmsg
    
    privileged: true
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 180s

  # =============================================================================
  # DASHBOARD AND MANAGEMENT
  # =============================================================================
  homer:
    image: b4bz/homer:latest
    container_name: homer
    restart: unless-stopped
    
    networks:
      - frontend
    
    ports:
      - "80:8080"
    
    volumes:
      - ./config/homer:/www/assets:ro
      - homer_data:/www/assets
    
    environment:
      - INIT_ASSETS=1
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
        reservations:
          memory: 32M
          cpus: '0.1'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s

  dashy:
    image: lissy93/dashy:2.1.1
    container_name: dashy
    restart: unless-stopped
    
    networks:
      - frontend
    
    ports:
      - "4000:80"
    
    volumes:
      - ./config/dashy/conf-simple.yml:/app/public/conf.yml:ro
      - dashy_data:/app/public/item-icons
    
    environment:
      - NODE_ENV=production
      - UID=1000
      - GID=1000
      - DISABLE_CONFIGURATION_WRITE=true
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.2'
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 5
        window: 300s

  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    restart: unless-stopped
    
    networks:
      - frontend
      - backend
    
    ports:
      - "9000:9000"
    
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9000/api/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s

  # =============================================================================
  # FILE MANAGEMENT AND STORAGE
  # =============================================================================
  filebrowser:
    image: filebrowser/filebrowser:latest
    container_name: filebrowser
    restart: unless-stopped
    
    networks:
      - frontend
    
    ports:
      - "8082:80"
    
    volumes:
      - ./data/files:/srv:rw
      - filebrowser_data:/database
      - ./config/filebrowser/filebrowser.json:/.filebrowser.json:ro
    
    environment:
      - FB_DATABASE=/database/filebrowser.db
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.2'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s

  # =============================================================================
  # PRODUCTIVITY APPLICATIONS
  # =============================================================================
  linkding:
    image: sissbruecker/linkding:latest
    container_name: linkding
    restart: unless-stopped
    
    networks:
      - frontend
      - backend
    
    ports:
      - "9091:9090"
    
    volumes:
      - linkding_data:/etc/linkding/data
    
    environment:
      - LD_SUPERUSER_NAME=${LINKDING_SUPERUSER_NAME:-admin}
      - LD_SUPERUSER_PASSWORD=${LINKDING_SUPERUSER_PASSWORD}
      - LD_DISABLE_BACKGROUND_TASKS=False
      - LD_DISABLE_URL_VALIDATION=False
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s

  actual:
    image: actualbudget/actual-server:latest
    container_name: actual
    restart: unless-stopped
    
    networks:
      - frontend
    
    ports:
      - "5006:5006"
    
    volumes:
      - actual_data:/data
    
    environment:
      - ACTUAL_PASSWORD=${ACTUAL_PASSWORD}
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:5006/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.3'
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s

  # =============================================================================
  # BACKUP SERVICES
  # =============================================================================
  duplicati:
    image: lscr.io/linuxserver/duplicati:latest
    container_name: duplicati
    restart: unless-stopped
    
    networks:
      - frontend
      - backend
    
    ports:
      - "8200:8200"
    
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=${TZ:-UTC}
      - CLI_ARGS=--webservice-interface=any --webservice-port=8200
    
    volumes:
      - duplicati_data:/config
      - ./data/backups:/backups
      - ./data:/source/data:ro
      - prometheus_data:/source/prometheus:ro
      - grafana_data:/source/grafana:ro
      - loki_data:/source/loki:ro
      - portainer_data:/source/portainer:ro
      - linkding_data:/source/linkding:ro
      - actual_data:/source/actual:ro
      - filebrowser_data:/source/filebrowser:ro
    
    depends_on:
      prometheus:
        condition: service_healthy
      grafana:
        condition: service_healthy
      portainer:
        condition: service_healthy
    
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8200/"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
        window: 300s

# =============================================================================
# NETWORKS CONFIGURATION
# =============================================================================
networks:
  # Frontend network - Services accessible through Cloudflare tunnel
  # This network allows external access via the tunnel while maintaining security
  frontend:
    name: homelab_frontend
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: ${DOCKER_SUBNET:-172.20.0.0/16}
          gateway: 172.20.0.1
    driver_opts:
      com.docker.network.bridge.name: homelab-frontend
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "true"
    labels:
      - "homelab.network.type=frontend"
      - "homelab.network.description=Frontend services accessible via Cloudflare tunnel"
      - "homelab.network.access=external"

  # Backend network - Internal service communication only
  # Isolated network for database and internal service communication
  backend:
    name: homelab_backend
    driver: bridge
    internal: true
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16
          gateway: 172.21.0.1
    driver_opts:
      com.docker.network.bridge.name: homelab-backend
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "false"
    labels:
      - "homelab.network.type=backend"
      - "homelab.network.description=Internal backend services communication"
      - "homelab.network.access=internal"

  # Monitoring network - Isolated monitoring stack
  # Dedicated network for metrics collection and log aggregation
  monitoring:
    name: homelab_monitoring
    driver: bridge
    internal: true
    ipam:
      driver: default
      config:
        - subnet: 172.22.0.0/16
          gateway: 172.22.0.1
    driver_opts:
      com.docker.network.bridge.name: homelab-monitoring
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "false"
    labels:
      - "homelab.network.type=monitoring"
      - "homelab.network.description=Monitoring stack isolation"
      - "homelab.network.access=internal"

# =============================================================================
# VOLUMES CONFIGURATION
# =============================================================================
volumes:
  # Infrastructure and tunnel volumes
  cloudflared_logs:
    name: homelab_cloudflared_logs
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/logs/cloudflared
    labels:
      - "homelab.volume.type=logs"
      - "homelab.volume.backup=optional"
      - "homelab.volume.network=frontend"

  # Monitoring stack volumes
  prometheus_data:
    name: homelab_prometheus_data
    driver: local
    labels:
      - "homelab.volume.type=metrics"
      - "homelab.volume.backup=important"
      - "homelab.volume.network=monitoring"
  
  grafana_data:
    name: homelab_grafana_data
    driver: local
    labels:
      - "homelab.volume.type=config"
      - "homelab.volume.backup=critical"
      - "homelab.volume.network=monitoring"
  
  loki_data:
    name: homelab_loki_data
    driver: local
    labels:
      - "homelab.volume.type=logs"
      - "homelab.volume.backup=optional"
      - "homelab.volume.network=monitoring"

  # Application data volumes
  portainer_data:
    name: homelab_portainer_data
    driver: local
    labels:
      - "homelab.volume.type=config"
      - "homelab.volume.backup=critical"
      - "homelab.volume.network=frontend,backend"
  
  homer_data:
    name: homelab_homer_data
    driver: local
    labels:
      - "homelab.volume.type=config"
      - "homelab.volume.backup=important"
      - "homelab.volume.network=frontend"
  
  dashy_data:
    name: homelab_dashy_data
    driver: local
    labels:
      - "homelab.volume.type=config"
      - "homelab.volume.backup=important"
      - "homelab.volume.network=frontend"
  
  filebrowser_data:
    name: homelab_filebrowser_data
    driver: local
    labels:
      - "homelab.volume.type=database"
      - "homelab.volume.backup=critical"
      - "homelab.volume.network=frontend"
  
  linkding_data:
    name: homelab_linkding_data
    driver: local
    labels:
      - "homelab.volume.type=database"
      - "homelab.volume.backup=critical"
      - "homelab.volume.network=frontend,backend"
  
  actual_data:
    name: homelab_actual_data
    driver: local
    labels:
      - "homelab.volume.type=database"
      - "homelab.volume.backup=critical"
      - "homelab.volume.network=frontend"
  
  duplicati_data:
    name: homelab_duplicati_data
    driver: local
    labels:
      - "homelab.volume.type=config"
      - "homelab.volume.backup=critical"
      - "homelab.volume.network=frontend,backend"

  # Shared volumes for cross-service communication
  shared_config:
    name: homelab_shared_config
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./config
    labels:
      - "homelab.volume.type=shared_config"
      - "homelab.volume.backup=critical"
      - "homelab.volume.network=all"
  
  shared_data:
    name: homelab_shared_data
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data
    labels:
      - "homelab.volume.type=shared_data"
      - "homelab.volume.backup=critical"
      - "homelab.volume.network=all"